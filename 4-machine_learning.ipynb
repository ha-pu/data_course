{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ha-pu/data_course/blob/AOM25/4-machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaiDKLvmk0jS"
      },
      "source": [
        "+ title: Machine Learning\n",
        "+ author: Harald Puhr\n",
        "+ date: July 20, 2025\n",
        "\n",
        "[Data Source](https://github.com/ha-pu/data_files#repurchase-likelihood-dataset)\n",
        "\n",
        "# Load packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "install.packages(c(\"caret\", \"caretEnsemble\", \"gbm\", \"glmnet\", \"randomForest\", \"tidyverse\", \"xgboost\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "library(caret)\n",
        "library(caretEnsemble)\n",
        "library(tidyverse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting a seed is important for reproducibility. It ensures that the random\n",
        "processes in your code yield the same results each time you run it. This is\n",
        "particularly crucial in machine learning, where randomness can significantly\n",
        "affect model training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "set.seed(123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "df <- read_csv(\"https://raw.githubusercontent.com/ha-pu/data_files/refs/heads/main/4-repurchase_likelihood.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing our data\n",
        "\n",
        "The implementations of most machine learning algorithms in R have very specific\n",
        "requirements regarding the coding of input data. This means we have to take\n",
        "more steps to prepare the data than when working with simple regressions.\n",
        "\n",
        "## Inspect data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "slice_head(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "glimpse(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "summary(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recode variables\n",
        "\n",
        "Code variables `repurchase`, `customer_sex`, and `customer_tier` as type factor\n",
        "to better handle categorical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "df <- df %>% \n",
        "  mutate(\n",
        "    repurchase = factor(repurchase),\n",
        "    customer_sex = factor(customer_sex),\n",
        "    customer_tier = factor(customer_tier)\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop columns which do not contain relevant information\n",
        "\n",
        "Our data set includes only one observation by customer. This means by including\n",
        "the variables `customer_id` and `customer_name`, we would create\n",
        "customer-specific predictions. This is not what we want to do! We want to\n",
        "generalize from our existing customers to new customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "df <- select(df, -customer_id, -customer_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform categorical predictors into dummy variables\n",
        "\n",
        "R stores categorical data as factors (e.g., variable = A, B, A, A, B). For\n",
        "machine learning, we transform them into separate columns:\n",
        "(variable.A = 1, 0, 1, 1, 0; variable.B = 0, 1, 0, 0, 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "dummyModel <- dummyVars(repurchase ~ ., df, fullRank = TRUE)\n",
        "\n",
        "df <- predict(dummyModel, newdata = df) %>%\n",
        "  as.data.frame() %>%\n",
        "  mutate(repurchase = df$repurchase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Designate he column names of our outcome and predictor variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "label <- \"repurchase\"\n",
        "predictors <- names(df)[!(names(df) %in% label)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splitting the data into training and test sets\n",
        "\n",
        "If we test the model on the same data as we train it, we can only verify how\n",
        "well the algorithm fits the data it has seen during training. This is not a good\n",
        "measure of accuracy. We want to know how well it fits data it has **not** seen\n",
        "in training. Therefore, we put part of the data aside for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "train_idx <- sample(seq(nrow(df)), size = nrow(df) * 0.8)\n",
        "train_df <- slice(df, train_idx)\n",
        "test_df <- slice(df, -train_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre-processing the data\n",
        "\n",
        "## Change data type of the label variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "y_train <- train_df[,label]\n",
        "y_test <- test_df[,label]\n",
        "\n",
        "# Ensure levels of y_train are valid R variable names\n",
        "levels(y_train) <- make.names(levels(y_train))\n",
        "levels(y_test) <- make.names(levels(y_test))\n",
        "\n",
        "head(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avoid multicollinearity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "findCorrelation(cor(train_df[,predictors]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check for missing values and use median imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "summary(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Machine learning algorithms do not handle missing data well. We, therefore,\n",
        "impute missing data with the sample median."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "preproc_par <- preProcess(train_df[,predictors], method = c(\"medianImpute\"))\n",
        "train_df[,predictors] <- predict(preproc_par, train_df[,predictors])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scale and center the data\n",
        "\n",
        "Machine learning algorithms do not handle differences in the scaling of\n",
        "variables well. This is an issue if we use variables that have different data\n",
        "ranges in the same model (e.g., Total GDP and GPD/capita). To solve the issue,\n",
        "we standardize numeric predictors to *mean = 0* and *SD = 1*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "preproc_par <- preProcess(train_df[,predictors], method=c(\"center\", \"scale\"))\n",
        "train_df[,predictors] <- predict(preproc_par, train_df[,predictors])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "preprocPar <- preProcess(test_df[,predictors], method=c(\"center\", \"scale\", \"medianImpute\"))\n",
        "test_df[,predictors] <- predict(preprocPar, test_df[,predictors])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model training, validation, and testing\n",
        "\n",
        "## Algorithms\n",
        "\n",
        "In this exercise, we will start with five frequently-used machine learning\n",
        "algorithms:\n",
        "\n",
        "* Random Forest (`method = \"rf\"`):\n",
        "  * What it does: Random Forest is an ensemble learning method that builds\n",
        "    multiple decision trees and merges their outputs for more accurate and\n",
        "    stable predictions.\n",
        "  * How it works: Each tree is trained on a random sample (with replacement) of\n",
        "    the data, and at each split in the tree, a random subset of features is\n",
        "    considered. The final prediction is made by majority vote (classification)\n",
        "    or average (regression).\n",
        "* Generalized Linear Model with Elastic Net Regularization (`method = \"glmnet\"`):\n",
        "  * What it does: This model combines ridge and lasso regression, useful for\n",
        "    handling multicollinearity and for variable selection.\n",
        "  * How it works: It minimizes the residual sum of squares with both L1 (lasso)\n",
        "    and L2 (ridge) penalties. The balance between these penalties is controlled\n",
        "    by a mixing parameter (`alpha`).\n",
        "* Neural Network (`method = \"nnet\"`):\n",
        "  * What it does: A simple feedforward neural network for classification or\n",
        "    regression.\n",
        "  * How it works: It uses one hidden layer with a customizable number of\n",
        "    neurons. The network learns weights for input features through\n",
        "    backpropagation and gradient descent, optimizing to reduce prediction error.\n",
        "* XGBoost with Decision Trees (`method = \"xgbTree\"`):\n",
        "  * What it does: XGBoost is an efficient implementation of gradient boosting\n",
        "    using decision trees.\n",
        "  * How it works: It builds trees sequentially, where each new tree corrects the\n",
        "    errors of the previous ones. It uses gradient descent to minimize a loss\n",
        "    function and includes regularization to prevent overfitting.\n",
        "* XGBoost with Linear Base Learners (`method = \"xgbLinear\"`):\n",
        "  * What it does: Like `xgbTree`, but uses linear models as base learners\n",
        "    instead of trees.\n",
        "  * How it works: It applies gradient boosting to linear models (like linear\n",
        "    regression), learning coefficients that minimize the loss function, with\n",
        "    regularization terms to avoid overfitting.\n",
        "\n",
        "## Training\n",
        "\n",
        "We use 5-fold cross-validation (CV) for training. CV means that the model is\n",
        "train on 4/5 of the data and validates it on the remaining 1/5 of the data. This\n",
        "process is repeated 4 times (once on every fold). Then, we take the average of\n",
        "these 5 evaluations to get the final evaluation of the respective training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "quickTrain <- trainControl(method = \"cv\", number = 5)\n",
        "\n",
        "model_rf <- train(x = train_df[,predictors], y = y_train, method = \"rf\", trControl = quickTrain)\n",
        "\n",
        "model_glmnet <- train(x = train_df[,predictors], y = y_train, method = \"glmnet\", trControl = quickTrain)\n",
        "\n",
        "model_nnet <- train(x = train_df[,predictors], y = y_train, method = \"nnet\", trControl = quickTrain)\n",
        "\n",
        "model_xgbTree <- train(x = train_df[,predictors], y = y_train, method = \"xgbTree\", trControl = quickTrain)\n",
        "\n",
        "model_xgbLinear <- train(x = train_df[,predictors], y = y_train, method = \"xgbLinear\", trControl = quickTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "model_glmnet %>%\n",
        "  ggplot(highlight = TRUE) +\n",
        "  theme_bw()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "model_nnet %>%\n",
        "  ggplot(highlight = TRUE) +\n",
        "  theme_bw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "comparison_alg <- data.frame(\n",
        "  Model = c(\"rf\",\"glmnet\",\"nnet\",\"xgbTree\",\"xgbLinear\"),\n",
        "  Accuracy_base = c(\n",
        "    max(model_rf$results$Accuracy), max(model_glmnet$results$Accuracy),\n",
        "    max(model_nnet$results$Accuracy), max(model_xgbTree$results$Accuracy),\n",
        "    max(model_xgbLinear$results$Accuracy))\n",
        ")\n",
        "\n",
        "comparison_alg %>%\n",
        "  ggplot() +\n",
        "  geom_bar(aes(x = Model, y = Accuracy_base), stat = \"identity\") +\n",
        "  geom_hline(aes(yintercept = max(Accuracy_base)), linetype = \"dashed\", color = \"darkred\") +\n",
        "  scale_y_continuous(limits = c(0.0, 0.9)) +\n",
        "  theme_bw()\n",
        "\n",
        "comparison_alg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional validation methods\n",
        "\n",
        "We repeat the analysis with 10-fold cross-validation and bootstrapping. In\n",
        "bootstrapping, we create sub-samples of the data by sampling with replacement.\n",
        "This means that some observations may be included multiple times in a\n",
        "sub-sample, while others may not be included at all. The model is trained on\n",
        "these sub-samples, and the process is repeated multiple times to obtain a more\n",
        "robust estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Cross validation training method\n",
        "crossValidation <- trainControl(method = \"cv\", number = 10)\n",
        "\n",
        "# Bootstrapping training method\n",
        "bootstraping <- trainControl(method = \"boot\")\n",
        "\n",
        "# Apply both methods to a random forest model\n",
        "model_cv <- train(x = train_df[,predictors], y = y_train, method = \"rf\", trControl = crossValidation)\n",
        "model_boot <- train(x = train_df[,predictors], y = y_train, method = \"rf\", trControl = bootstraping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "comparison_val <- data.frame(\n",
        "  Model = c(\"cv5\", \"cv10\",\"boot\"),\n",
        "  Accuracy_base = c(\n",
        "    max(model_rf$results$Accuracy), max(model_cv$results$Accuracy),\n",
        "    max(model_boot$results$Accuracy)\n",
        "  )\n",
        ")\n",
        "\n",
        "comparison_val %>%\n",
        "  ggplot() +\n",
        "  geom_bar(aes(x = Model, y = Accuracy_base), stat = \"identity\") +\n",
        "  geom_hline(aes(yintercept = max(Accuracy_base)), linetype = \"dashed\", color = \"darkred\") +\n",
        "  scale_y_continuous(limits = c(0.0, 0.9)) +\n",
        "  theme_bw()\n",
        "\n",
        "comparison_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model tuning\n",
        "\n",
        "Through tuning, we can optimize the hyper-parameters of our model to improve its\n",
        "learning behavior. Hyper-parameters are parameters that are set before the\n",
        "learning process begins. They are not learned from the data but are instead\n",
        "specified by the user before training the model. Examples include\n",
        "the learning rate, the number of trees in a random forest, the number of hidden\n",
        "layers in a neural network, and the regularization strength in a linear model.\n",
        "\n",
        "We test three approaches to tuning:\n",
        "\n",
        "* Grid search: Systematically evaluates a predefined grid of hyper-parameter\n",
        "  combinations.\n",
        "* Random search: Randomly samples parameter combinations from the parameter\n",
        "  space.\n",
        "* Adaptive search: Starts with a random search (or grid), but learns from\n",
        "  performance during tuning and stops testing unpromising combinations early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Grid tuning\n",
        "gridTune <- trainControl(method = \"cv\", search = \"grid\")\n",
        "\n",
        "# Random search tuning method:\n",
        "randomTune <- trainControl(method = \"cv\", search = \"random\")\n",
        "\n",
        "# Adaptive search tuning method:\n",
        "adaptiveTune <- trainControl(\n",
        "  method = \"adaptive_cv\",\n",
        "  adaptive = list(min = 2, alpha = 0.05, method = \"gls\", complete = TRUE),\n",
        "  search = \"random\"\n",
        "  )\n",
        "\n",
        "# Apply all three methods to a glmnet model\n",
        "model_grid <- train(\n",
        "  x = train_df[,predictors], y = y_train, method = \"glmnet\",\n",
        "  trControl = gridTune, tuneLength = 5\n",
        "  )\n",
        "model_random <- train(\n",
        "  x = train_df[,predictors], y = y_train, method = \"glmnet\",\n",
        "  trControl = randomTune, tuneLength = 25\n",
        "  )\n",
        "model_adaptive <- train(\n",
        "  x = train_df[,predictors], y = y_train, method = \"glmnet\",\n",
        "  trControl = adaptiveTune, tuneLength = 25\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "comparison_tune <- data.frame(\n",
        "  Model = c(\"cv5\", \"grid\", \"random\", \"adaptive\"),\n",
        "  Accuracy_base = c(\n",
        "    max(model_glmnet$results$Accuracy), max(model_grid$results$Accuracy),\n",
        "    max(model_random$results$Accuracy), max(model_adaptive$results$Accuracy)\n",
        "  )\n",
        ")\n",
        "\n",
        "comparison_tune %>%\n",
        "  ggplot() +\n",
        "  geom_bar(aes(x = Model, y = Accuracy_base), stat = \"identity\") +\n",
        "  geom_hline(aes(yintercept = max(Accuracy_base)), linetype = \"dashed\", color = \"darkred\") +\n",
        "  scale_y_continuous(limits = c(0.0, 0.9)) +\n",
        "  theme_bw()\n",
        "\n",
        "comparison_tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "comparison_all <- bind_rows(comparison_alg, comparison_val, comparison_tune) %>%\n",
        "  filter(Model != \"cv5\")\n",
        "\n",
        "det_OOS_accuracy <- function(model){\n",
        "  confusionMatrix(predict(model, test_df), y_test)$overall[[1]]\n",
        "}\n",
        "\n",
        "all_model_list <- list(\n",
        "  model_rf, model_glmnet, model_nnet, model_xgbTree,\n",
        "  model_xgbLinear, model_cv, model_boot, model_grid,\n",
        "  model_random, model_adaptive\n",
        ")\n",
        "\n",
        "comparison_all <- mutate(\n",
        "  comparison_all,\n",
        "  OOS_Acc = map_dbl(all_model_list, det_OOS_accuracy)\n",
        ")\n",
        "\n",
        "comparison_all %>%\n",
        "  pivot_longer(\n",
        "    cols = c(Accuracy_base, OOS_Acc),\n",
        "    names_to = \"Base\",\n",
        "    values_to = \"Accuracy\"\n",
        "  ) %>%\n",
        "  ggplot() +\n",
        "  geom_line(aes(x = Model, y = Accuracy, color = Base, group = Base)) +\n",
        "  geom_point(aes(x = Model, y = Accuracy, color = Base)) +\n",
        "  geom_hline(aes(yintercept = max(comparison_all$Accuracy_base)), colour = \"darkred\", linetype = \"dashed\") +\n",
        "  geom_hline(aes(yintercept = max(comparison_all$OOS_Acc)), colour = \"darkblue\", linetype = \"dashed\") +\n",
        "  scale_y_continuous(limits = c(0.0, 0.9)) +\n",
        "  theme_bw()\n",
        "\n",
        "arrange(comparison_all, desc(OOS_Acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variable importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "varImp(model_xgbTree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "plot(varImp(model_xgbTree), main = \"xgbTree\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "plot(varImp(model_nnet), main = \"nnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "plot(varImp(model_glmnet), main = \"glmnet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble learning\n",
        "\n",
        "Ensemble learning is a machine learning technique that combines predictions from\n",
        "multiple models to improve accuracy, robustness, and generalization. The idea is\n",
        "simple but powerful: A group of diverse models working together is usually\n",
        "better than any single one.\n",
        "\n",
        "There are mulitpe ways to combine models:\n",
        "* Bagging: Build many models on different random subsets of the data and combine\n",
        "  them via averaging (regression) or voting (classification).\n",
        "* Boosting: Build models sequentially, where each model learns from the errors\n",
        "  of the previous and combine them with weighted averaging.\n",
        "* Stacking: Build several base models and train a meta-model on their predictions.\n",
        "\n",
        "We first define a list of models and apply them to the same folds of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "algoList <- c(\"rf\", \"glmnet\", \"nnet\", \"xgbTree\", \"xgbLinear\")\n",
        "\n",
        "stack_index = createFolds(y_train, k = 5) # We want to fix our index for comparison and validity\n",
        "\n",
        "listControl <- trainControl(\n",
        "  method = \"cv\", number = 5, \n",
        "  classProbs = TRUE, # We need to predict probabilities not 0 or 1\n",
        "  savePredictions = TRUE, # necessary for stack model later\n",
        "  index = stack_index\n",
        ")\n",
        "\n",
        "models <- caretList(\n",
        "  x = train_df[,predictors], y = y_train, trControl = listControl,\n",
        "  methodList = algoList\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "results <- resamples(models)\n",
        "summary(results)\n",
        "modelCor(resamples(models))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic model selection\n",
        "\n",
        "Next, we use a meta model to combine the predictions of the base models. The\n",
        "meta model can be a simple linear regression model or a more complex model like\n",
        "an extreme gradient boosting tree (`xgbTree`), a gradient boosting machine\n",
        "(`gbm`), or a simple logistic regression (`glm`). GBM is an ensemble learning\n",
        "method that builds a strong predictive model by sequentially combining many weak\n",
        "models, typically decision trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "stackControl <- trainControl(\n",
        "  method = \"repeatedcv\", number = 10, repeats = 3, classProbs = TRUE,\n",
        "  index = stack_index\n",
        "  )\n",
        "\n",
        "stack_tree <- caretStack(models, method = \"xgbTree\", trControl = stackControl)\n",
        "\n",
        "stack_gbm <- caretStack(models, method = \"gbm\", trControl = stackControl)\n",
        "\n",
        "stack_glm <- caretStack(models, method = \"glm\", trControl = stackControl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## Out-of-Sample Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "results <- data.frame(\n",
        "  ensemble_tree = predict(stack_tree, test_df)$X1,\n",
        "  ensemble_glm = predict(stack_glm, test_df)$X1,\n",
        "  ensemble_gbm = predict(stack_gbm, test_df)$X1\n",
        ")\n",
        "\n",
        "results <- predict(models, test_df) %>%\n",
        "  as.data.frame() %>% \n",
        "  bind_cols(results)\n",
        "\n",
        "det_OOS_accuracy_probs <- function(model){\n",
        "  preds <- factor(model >= 0.5, labels = c(\"X0\", \"X1\"))\n",
        "  confusionMatrix(preds, y_test)$overall[[1]]\n",
        "}\n",
        "\n",
        "OOS_ACC <- results %>%\n",
        "  summarise(across(everything(), det_OOS_accuracy_probs))\n",
        "\n",
        "OOS_ACC\n",
        "\n",
        "OOS_ACC %>%\n",
        "  pivot_longer(cols = everything(), names_to = \"Model\", values_to = \"OOS_ACC\") %>%\n",
        "  ggplot() +\n",
        "  geom_bar(aes(x = Model, y = OOS_ACC), stat = \"identity\") + \n",
        "  geom_hline(aes(yintercept = max(OOS_ACC)), linetype = \"dashed\", color = \"darkred\") +\n",
        "  labs(\n",
        "    x = NULL\n",
        "  ) +\n",
        "  theme_bw() +\n",
        "  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN32YnVDkBH6+AOyEXOXkIM",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
